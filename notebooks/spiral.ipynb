{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 3000\n",
    "num_classes = 3\n",
    "\n",
    "# Generate random data\n",
    "\n",
    "def p(t, f=0):\n",
    "    x = (1-t)*torch.vstack([\n",
    "            torch.cos(2*torch.pi*t + f),\n",
    "            torch.sin(2*torch.pi*t + f)\n",
    "        ]) + torch.randn(2, len(t))*0.03\n",
    "    return torch.transpose(x, 0, 1)\n",
    "\n",
    "X, Y = [], []\n",
    "a = 2*torch.pi/num_classes\n",
    "samples_per_class = num_samples//num_classes\n",
    "for c in range(num_classes):\n",
    "    t = torch.linspace(0, 0.9, samples_per_class)\n",
    "    X.append( p(t, c*a) )\n",
    "    # y is one hot encoded\n",
    "    y = torch.zeros((samples_per_class, num_classes), dtype=torch.float32)\n",
    "    y[:, c] = 1\n",
    "    Y.append(y)\n",
    "X = torch.vstack(X)\n",
    "Y = torch.vstack(Y)\n",
    "# randomize 3 times X and Y\n",
    "for i in range(3):\n",
    "    idx = torch.randperm(num_samples)\n",
    "    X = X[idx]\n",
    "    Y = Y[idx]\n",
    "\n",
    "plt.figure()\n",
    "for c in range(num_classes):\n",
    "    idx = torch.where(Y[:,c]==1)[0]\n",
    "    plt.scatter(X[idx,0], X[idx,1], s=1)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train, Validatio and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = math.floor(num_samples*0.7)\n",
    "num_validation_samples = math.floor(num_samples*0.2)\n",
    "num_test_samples = num_samples - num_train_samples - num_validation_samples\n",
    "\n",
    "X_train, Y_train = X[:num_train_samples], Y[:num_train_samples]\n",
    "X_validation, Y_validation = X[num_train_samples:num_train_samples+num_validation_samples], Y[num_train_samples:num_train_samples+num_validation_samples]\n",
    "X_test, Y_test = X[num_train_samples+num_validation_samples:], Y[num_train_samples+num_validation_samples:]\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_validation.shape, Y_validation.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 10),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(10, num_classes),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for i in range(0, num_train_samples, batch_size):\n",
    "        X_batch = X_train[i:i+batch_size].to(device)\n",
    "        Y_batch = Y_train[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = loss_fn(Y_pred, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()*len(X_batch)\n",
    "\n",
    "    train_losses.append(loss_val/num_train_samples)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Y_pred = model(X_validation)\n",
    "        loss = loss_fn(Y_pred, Y_validation)\n",
    "\n",
    "        validation_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch}, loss: {loss.item()}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(validation_losses, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     Y_pred = model(X_test)\n",
    "#     loss = criterion(Y_pred, Y_test)\n",
    "#     print(f\"Test loss: {loss.item()}\")\n",
    "\n",
    "#     Y_pred = torch.argmax(Y_pred, dim=1)\n",
    "#     Y_test = torch.argmax(Y_test, dim=1)\n",
    "#     accuracy = torch.sum(Y_pred == Y_test).item() / num_test_samples\n",
    "#     print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "#     plt.figure()\n",
    "#     for c in range(num_classes):\n",
    "#         idx = torch.where(Y_test==c)[0]\n",
    "#         plt.scatter(X_test[idx,0], X_test[idx,1], s=1, c=Y_pred[idx].numpy())\n",
    "#     plt.show(\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
